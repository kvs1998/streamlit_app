# tabs/dt_health_tab.py (Simplified global filter application)
import streamlit as st
import pandas as pd
import plotly.express as px
from datetime import datetime, timedelta

# --- Helper functions (from previous code) ---
def format_seconds_to_readable(seconds_series, format_type):
    if not isinstance(seconds_series, pd.Series):
        seconds_series = pd.Series([seconds_series])
    numeric_series = pd.to_numeric(seconds_series, errors='coerce')
    if numeric_series.empty or pd.isna(numeric_series).all():
        return pd.Series(["N/A"] * len(numeric_series), index=numeric_series.index)
    def format_single_second(s, f_type):
        if pd.isna(s): return "N/A"
        s = float(s)
        if s == 0: return "0s"
        if f_type == "seconds": return f"{s:.1f}s"
        elif f_type == "minutes": return f"{(s / 60):.1f}m"
        elif f_type == "hours": return f"{(s / 3600):.1f}h"
        elif f_type == "days": return f"{(s / 86400):.1f}d"
        elif f_type == "mixed":
            days = int(s // 86400); s_remaining = s % 86400
            hours = int(s_remaining // 3600); s_remaining = s_remaining % 3600
            minutes = int(s_remaining // 60); seconds = s_remaining % 60
            parts = []
            if days > 0: parts.append(f"{days}d")
            if hours > 0: parts.append(f"{hours}h")
            if minutes > 0: parts.append(f"{minutes}m")
            if seconds > 0.1 and (not parts or seconds >= 1): parts.append(f"{seconds:.1f}s")
            return " ".join(parts) if parts else "0s"
        return str(s)
    return numeric_series.apply(lambda x: format_single_second(x, f_type=format_type))

# --- Data Fetching Functions ---

@st.cache_data(ttl=(12*60*60))
def fetch_driver_execution_history_summary():
    session = st.session_state.get_snowflake_session()
    AUDIT_LOG_TABLE_FQDN = "YOUR_DB.YOUR_SCHEMA.T_DT_COLLECTION_AUDIT_LOG"
    query = f"""
    SELECT
        DRIVER_RUN_UUID, RUN_STATUS, RUN_START_TIME, RUN_END_TIME,
        TOTAL_DURATION_SEC, TOTAL_TABLES_FOUND, TOTAL_JOBS_LAUNCHED,
        TOTAL_ASYNC_JOBS_SUCCEEDED, TOTAL_ASYNC_JOBS_FAILED,
        MESSAGE AS RUN_MESSAGE, TIMEZONE
    FROM {AUDIT_LOG_TABLE_FQDN}
    WHERE RUN_START_TIME >= DATEADD(day, -30, CURRENT_TIMESTAMP())
    ORDER BY RUN_START_TIME DESC
    """
    df = session.sql(query).to_pandas()
    if not df.empty:
        df['RUN_START_TIME'] = pd.to_datetime(df['RUN_START_TIME'])
        df['RUN_END_TIME'] = pd.to_datetime(df['RUN_END_TIME'])
    return df

@st.cache_data(ttl=(5*60))
def fetch_dt_tracking_data():
    session = st.session_state.get_snowflake_session()
    TRACKING_TABLE_FQDN = "YOUR_DB.YOUR_SCHEMA.T_DYNAMIC_TABLE_TRACKING"
    METADATA_SNAPSHOT_TABLE_FQDN = "YOUR_DB.YOUR_SCHEMA.T_DYNAMIC_TABLE_METADATA_LATEST_SNAPSHOT"

    query = f"""
    SELECT
        t.QUALIFIED_NAME,
        t.DATABASE_NAME,
        t.SCHEMA_NAME,
        t.TABLE_NAME,
        t.DOMAIN_NAME,
        t.SUB_DOMAIN_NAME,
        t.IS_ACTIVE,
        t.TRACK_REFRESH_HISTORY,
        t.LAST_REFRESH_HISTORY_COLLECTION_TIMESTAMP,
        t.LAST_REFRESH_HISTORY_COLLECTION_STATUS,
        t.LAST_REFRESH_HISTORY_COLLECTION_MESSAGE,
        t.TRACK_METADATA_SNAPSHOT,
        t.LAST_METADATA_COLLECTION_TIMESTAMP,
        t.LAST_METADATA_COLLECTION_STATUS,
        t.LAST_METADATA_COLLECTION_MESSAGE,
        ms.LATEST_DATA_TIMESTAMP,
        ms.MEAN_LAG_SEC,
        ms.MAXIMUM_LAG_SEC,
        ms.TARGET_LAG_SEC,
        t.UPDATED_AT
    FROM
        {TRACKING_TABLE_FQDN} AS t
    LEFT JOIN
        {METADATA_SNAPSHOT_TABLE_FQDN} AS ms
    ON
        t.QUALIFIED_NAME = ms.QUALIFIED_NAME
    ORDER BY
        t.QUALIFIED_NAME
    """
    df = session.sql(query).to_pandas()
    if not df.empty:
        df['LAST_REFRESH_HISTORY_COLLECTION_TIMESTAMP'] = pd.to_datetime(df['LAST_REFRESH_HISTORY_COLLECTION_TIMESTAMP'])
        df['LAST_METADATA_COLLECTION_TIMESTAMP'] = pd.to_datetime(df['LAST_METADATA_COLLECTION_TIMESTAMP'])
        df['LATEST_DATA_TIMESTAMP'] = pd.to_datetime(df['LATEST_DATA_TIMESTAMP'])
        df['UPDATED_AT'] = pd.to_datetime(df['UPDATED_AT'])
    return df


# No need for persist_widget_state, load_widget_state in this file anymore if only global filters affect it
# or if local filters are simplified.

# --- Main Render Function for the DT Health Tab ---
def render_dt_health_tab():
    st.header("Dynamic Table Health Dashboard")
    st.write("Overview of driver execution, collection status, and detailed table health.")

    # --- 1. Fetch Data ---
    driver_history_df = fetch_driver_execution_history_summary()
    all_tracking_data_df = fetch_dt_tracking_data()

    if all_tracking_data_df.empty:
        st.info("No data available from tracking tables. Please check data sources.", icon="ℹ️")
        return
    
    # --- Apply GLOBAL Filters (First Level of Filtering) from session_state ---
    # These values are set by filter_canvas.py
    tracking_data_df = all_tracking_data_df.copy() # Work on a copy

    # Ensure global filter states are initialized with a default fallback if filter_canvas wasn't visited
    global_domain_name = st.session_state.get('global_domain_name', 'All')
    global_sub_domain_name = st.session_state.get('global_sub_domain_name', 'All')
    global_db_filter = st.session_state.get('global_db_filter', 'All')
    global_schema_filter = st.session_state.get('global_schema_filter', 'All')
    global_table_filter = st.session_state.get('global_table_filter', ['All'])
    global_scheduling_state_filter = st.session_state.get('global_scheduling_state_filter', ['All'])
    global_target_lag_filter = st.session_state.get('global_target_lag', ['All'])
    display_lag_times_in = st.session_state.get('display_lag_times_in', 'mixed') # Also part of global filters now
    global_is_active_filter = st.session_state.get('global_is_active_filter', 'All')


    # Apply filters in dependency order to the DataFrame
    if global_domain_name != 'All':
        tracking_data_df = tracking_data_df[tracking_data_df['DOMAIN_NAME'] == global_domain_name]
    if global_sub_domain_name != 'All':
        tracking_data_df = tracking_data_df[tracking_data_df['SUB_DOMAIN_NAME'] == global_sub_domain_name]
    if global_db_filter != 'All':
        tracking_data_df = tracking_data_df[tracking_data_df['DATABASE_NAME'] == global_db_filter]
    if global_schema_filter != 'All':
        tracking_data_df = tracking_data_df[tracking_data_df['SCHEMA_NAME'] == global_schema_filter]
    if 'All' not in global_table_filter:
        tracking_data_df = tracking_data_df[tracking_data_df['TABLE_NAME'].isin(global_table_filter)]
    if 'All' not in global_scheduling_state_filter:
        tracking_data_df = tracking_data_df[tracking_data_df['LAST_REFRESH_HISTORY_COLLECTION_STATUS'].isin(global_scheduling_state_filter)]
    if 'All' not in global_target_lag_filter:
        target_lag_values_for_filter_numeric = [int(s.replace('s', '')) for s in global_target_lag_filter]
        tracking_data_df = tracking_data_df[tracking_data_df['TARGET_LAG_SEC'].isin(target_lag_values_for_filter_numeric)]
    if global_is_active_filter != 'All':
        tracking_data_df = tracking_data_df[tracking_data_df['IS_ACTIVE'] == global_is_active_filter] # This is boolean already


    if tracking_data_df.empty:
        st.info("No data available based on the current Global Filter Canvas selections. Please adjust your global filters.", icon="ℹ️")
        return


    # --- 2. Overall Driver Health KPIs ---
    st.subheader("Overall Driver Health KPIs")
    kpi_cols = st.columns(4)

    latest_run = driver_history_df.iloc[0] if not driver_history_df.empty else None
    
    total_tables_tracked = tracking_data_df['QUALIFIED_NAME'].nunique() if not tracking_data_df.empty else 0
    active_rh_tracking = tracking_data_df[
        (tracking_data_df['IS_ACTIVE'] == True) & (tracking_data_df['TRACK_REFRESH_HISTORY'] == True)
    ].shape[0] if not tracking_data_df.empty else 0
    active_metadata_tracking = tracking_data_df[
        (tracking_data_df['IS_ACTIVE'] == True) & (tracking_data_df['TRACK_METADATA_SNAPSHOT'] == True)
    ].shape[0] if not tracking_data_df.empty else 0
    
    tables_not_refreshed_last_3_days = 0
    if not tracking_data_df.empty and 'LATEST_DATA_TIMESTAMP' in tracking_data_df.columns:
        three_days_ago = datetime.now() - timedelta(days=3)
        tables_not_refreshed_last_3_days = tracking_data_df[
            (tracking_data_df['LATEST_DATA_TIMESTAMP'].isna()) |
            (tracking_data_df['LATEST_DATA_TIMESTAMP'] < three_days_ago)
        ].shape[0]

    with kpi_cols[0]:
        st.metric("Total Tables Tracked", total_tables_tracked)
        st.metric("Active (RH) Tracking", active_rh_tracking)
    with kpi_cols[1]:
        st.metric("Active Tables Monitored", total_tables_tracked)
        st.metric("Active (Metadata) Tracking", active_metadata_tracking)
    with kpi_cols[2]:
        if latest_run is not None:
            st.metric("Last Driver Run Time", latest_run['RUN_END_TIME'].strftime('%Y-%m-%d %H:%M:%S'))
        else:
            st.metric("Last Driver Run Time", "N/A")
        st.metric("Tables Not Refreshed (last 3d)", tables_not_refreshed_last_3_days)
    with kpi_cols[3]:
        if latest_run is not None:
            duration_formatted = format_seconds_to_readable(
                pd.Series([latest_run['TOTAL_DURATION_SEC']]), "mixed"
            ).iloc[0]
            st.metric("Last Run Duration", duration_formatted)
        else:
            st.metric("Last Run Duration", "N/A")

    st.markdown("---")

    # --- 3. Driver Execution Time Trend Chart and Collection Status Distribution ---
    chart_row_cols = st.columns([0.6, 0.4])

    with chart_row_cols[0]: # Driver Execution Time Trend Chart (Left)
        st.subheader("Driver Execution Time Trend")
        st.write("Historical execution times of the main collection driver program.")
        if not driver_history_df.empty:
            driver_history_chart_data = driver_history_df.copy()
            driver_history_chart_data.rename(columns={
                'RUN_START_TIME': 'Run Time',
                'TOTAL_DURATION_SEC': 'Duration (seconds)'
            }, inplace=True)

            log_scale_y = st.checkbox("Log Scale Y-axis (Driver Trend)", value=False, key="log_scale_driver_trend")
            
            fig_trend = px.line(
                driver_history_chart_data,
                x='Run Time',
                y='Duration (seconds)',
                title='Driver Program Execution Duration Over Time',
                markers=True,
                log_y=log_scale_y
            )
            fig_trend.update_layout(hovermode="x unified")
            st.plotly_chart(fig_trend, use_container_width=True)
        else:
            st.info("No historical driver execution data for trend chart.", icon="ℹ️")

    with chart_row_cols[1]: # Collection Status Distribution (Right)
        st.subheader("Collection Status Distribution")
        st.write("Breakdown of tables by their last collection status for each type.")
        if not tracking_data_df.empty:
            rh_status_counts = tracking_data_df['LAST_REFRESH_HISTORY_COLLECTION_STATUS'].value_counts().reset_index()
            rh_status_counts.columns = ['Status', 'Count']
            rh_status_counts['Collection Type'] = 'Refresh History'

            md_status_counts = tracking_data_df['LAST_METADATA_COLLECTION_STATUS'].value_counts().reset_index()
            md_status_counts.columns = ['Status', 'Count']
            md_status_counts['Collection Type'] = 'Metadata Snapshot'

            combined_status_counts = pd.concat([rh_status_counts, md_status_counts])

            status_color_map_for_chart = {
                'SUCCESS': 'green',
                'SUCCESS_NO_RECORDS': 'lightgreen',
                'FAILED': 'red',
                'FAILED_EXECUTION': 'red',
                'FAILED_INPUT': 'red',
                'FAILED_SUBMISSION': 'red',
                'SUSPENDED': 'orange',
                'UNKNOWN': 'gray',
                'N/A': 'lightgray'
            }

            fig_bar = px.bar(
                combined_status_counts,
                x='Count',
                y='Collection Type',
                color='Status',
                orientation='h',
                title='Last Collection Status by Type',
                color_discrete_map=status_color_map_for_chart,
                text='Count'
            )
            fig_bar.update_traces(textposition='outside')
            fig_bar.update_layout(showlegend=True, yaxis_title=None, xaxis_title='Number of Tables')
            st.plotly_chart(fig_bar, use_container_width=True)
        else:
            st.info("No detailed tracking data for status distribution.", icon="ℹ️")
    
    st.markdown("---")

    # --- 4. Detailed Collection Status per Table (with no local filters anymore) ---
    st.subheader("Detailed Collection Status per Table")
    st.write("View the last collection attempt status and messages for each dynamic table being tracked.")

    # In this simplified model, all filtering is done via the global filters in filter_canvas.py
    # So, tracking_data_df already holds the final filtered data.
    
    if tracking_data_df.empty:
        st.info("No detailed collection status per table available.", icon="ℹ️")
        return

    # --- Display Detailed Table ---
    display_df = tracking_data_df.copy()

    # Reformat timestamps for display
    display_df['LAST_RH_COLLECT_TIME'] = display_df['LAST_REFRESH_HISTORY_COLLECTION_TIMESTAMP'].dt.strftime('%Y-%m-%d %H:%M:%S')
    display_df['LAST_MD_COLLECT_TIME'] = display_df['LAST_METADATA_COLLECTION_TIMESTAMP'].dt.strftime('%Y-%m-%d %H:%M:%S')
    display_df['LATEST_DATA_TIMESTAMP_FMT'] = display_df['LATEST_DATA_TIMESTAMP'].dt.strftime('%Y-%m-%d %H:%M:%S')
    display_df['TRACKING_RECORD_LAST_UPDATED'] = display_df['UPDATED_AT'].dt.strftime('%Y-%m-%d %H:%M:%S')

    # Apply time formatting for lag columns based on the global display_lag_times_in filter
    if 'MEAN_LAG_SEC' in display_df.columns:
        display_df['MEAN_LAG_SEC_FMT'] = format_seconds_to_readable(display_df['MEAN_LAG_SEC'], display_lag_times_in)
    if 'MAXIMUM_LAG_SEC' in display_df.columns:
        display_df['MAXIMUM_LAG_SEC_FMT'] = format_seconds_to_readable(display_df['MAXIMUM_LAG_SEC'], display_lag_times_in)
    if 'TARGET_LAG_SEC' in display_df.columns:
        display_df['TARGET_LAG_SEC_FMT'] = format_seconds_to_readable(display_df['TARGET_LAG_SEC'], display_lag_times_in)


    # Drop message columns
    display_df = display_df.drop(columns=[
        'LAST_REFRESH_HISTORY_COLLECTION_MESSAGE', 
        'LAST_METADATA_COLLECTION_MESSAGE'
    ], errors='ignore') 

    # Rename columns for presentation
    display_df = display_df.rename(columns={
        'QUALIFIED_NAME': 'Dynamic Table',
        'IS_ACTIVE': 'Active?',
        'TRACK_REFRESH_HISTORY': 'Track RH?',
        'LAST_REFRESH_HISTORY_COLLECTION_STATUS': 'Last RH Status',
        'TRACK_METADATA_SNAPSHOT': 'Track Metadata?',
        'LAST_METADATA_COLLECTION_STATUS': 'Last Metadata Status',
        'LAST_RH_COLLECT_TIME': 'Last RH Collect Time',
        'LAST_MD_COLLECT_TIME': 'Last Metadata Collect Time',
        'LATEST_DATA_TIMESTAMP': 'Latest Data Time',
        'UPDATED_AT': 'Tracking Record Last Updated'
    })

    # Select and reorder columns for final display
    final_cols_order = [
        'Dynamic Table',
        'Active?',
        'Track RH?',
        'Last RH Collect Time',
        'Last RH Status',
        'Track Metadata?',
        'Last Metadata Collect Time',
        'Last Metadata Status',
        'Latest Data Time',
        'MEAN_LAG_SEC_FMT',
        'MAXIMUM_LAG_SEC_FMT',
        'TARGET_LAG_SEC_FMT',
        'Tracking Record Last Updated'
    ]

    final_display_df = display_df[[col for col in final_cols_order if col in display_df.columns]]

    st.dataframe(final_display_df, use_container_width=True)
