CREATE OR REPLACE TABLE YOUR_DB.YOUR_SCHEMA.T_DT_COLLECTION_AUDIT_LOG (
    DRIVER_RUN_UUID VARCHAR PRIMARY KEY,
    RUN_STATUS VARCHAR, -- 'SUCCESS', 'FAILED_OVERALL', 'CRITICAL_FAILURE'
    RUN_START_TIME TIMESTAMP_LTZ,
    RUN_END_TIME TIMESTAMP_LTZ,
    TOTAL_DURATION_SEC NUMBER(10,3),
    TOTAL_TABLES_FOUND INTEGER,
    TOTAL_JOBS_LAUNCHED INTEGER,
    TOTAL_ASYNC_JOBS_SUCCEEDED INTEGER,
    TOTAL_ASYNC_JOBS_FAILED INTEGER,
    AUDIT_JSON VARIANT, -- Stores the complete JSON object returned by the SP
    LOAD_TIMESTAMP TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP()
);

-- Optional: Add a clustering key if you expect many rows and want faster lookups by time
ALTER TABLE YOUR_DB.YOUR_SCHEMA.T_DT_COLLECTION_AUDIT_LOG CLUSTER BY (RUN_START_TIME::DATE);



-- Replace placeholders YOUR_DB, YOUR_SCHEMA, YOUR_WAREHOUSE
-- Adjust schedule, batch size as needed.

CREATE OR REPLACE TASK YOUR_DB.YOUR_SCHEMA.DAILY_DT_COLLECTION_TASK
  WAREHOUSE = YOUR_WAREHOUSE_NAME
  SCHEDULE = 'USING CRON 0 3 * * * America/Los_Angeles'
  ALLOW_OVERLAPPING_EXECUTION = FALSE
  USER_TASK_TIMEOUT_MS = 14400000
  SUSPEND_TASK_AFTER_NUM_FAILURES = 3
  COMMENT = 'Orchestrates the collection of Dynamic Table metadata and refresh history and logs audit to T_DT_COLLECTION_AUDIT_LOG.'
  LOG_LEVEL = 'INFO'
AS
$$
    -- Snowflake Scripting block within the task
    DECLARE
        sp_result VARIANT; -- Declare a variable to hold the JSON output
    BEGIN
        -- Call your main driver stored procedure
        sp_result := CALL YOUR_DB.YOUR_SCHEMA.SP_DRIVE_ALL_DT_COLLECTION(
            P_ASYNC_BATCH_SIZE => 50, -- Your chosen batch size
            P_TIMEZONE_NAME => 'America/Los_Angeles' -- Pass the timezone to the SP
        );

        -- Insert the parsed components of the JSON output into your audit table
        INSERT INTO YOUR_DB.YOUR_SCHEMA.T_DT_COLLECTION_AUDIT_LOG (
            DRIVER_RUN_UUID,
            RUN_STATUS,
            RUN_START_TIME,
            RUN_END_TIME,
            TOTAL_DURATION_SEC,
            TOTAL_TABLES_FOUND,
            TOTAL_JOBS_LAUNCHED,
            TOTAL_ASYNC_JOBS_SUCCEEDED,
            TOTAL_ASYNC_JOBS_FAILED,
            AUDIT_JSON
        )
        SELECT
            sp_result:driver_run_uuid::VARCHAR,
            sp_result:status::VARCHAR,
            sp_result:start_time::TIMESTAMP_LTZ,
            sp_result:end_time::TIMESTAMP_LTZ,
            sp_result:total_run_duration_sec::NUMBER(10,3),
            sp_result:total_tables_found::INTEGER,
            sp_result:total_jobs_launched::INTEGER,
            sp_result:total_async_jobs_succeeded::INTEGER,
            sp_result:total_async_jobs_failed::INTEGER,
            sp_result -- Store the entire JSON object for full detail
        ;
        
        -- Optional: Add auditing for task success/failure based on sp_result:status
        -- e.g., if sp_result:status is 'FAILED_OVERALL', you could log to a separate simple error table
        -- or trigger an alert using an ERROR_INTEGRATION on the task.
        
        -- Tasks implicitly commit DMLs by default. If the insert fails, the task will fail.
    END;
$$;

-- After creation, remember to resume the task:
ALTER TASK YOUR_DB.YOUR_SCHEMA.DAILY_DT_COLLECTION_TASK RESUME;




import pandas as pd
# Assuming get_snowflake_session() is defined elsewhere and works
# from snowflake.snowpark import Session # if using Snowpark for session
# from snowflake.connector import connect # if using Connector for session
# ... your session creation logic

@st.cache_data(ttl=(12*60*60)) # Cache for 12 hours
def fetch_driver_execution_history():
    """
    Fetches execution history of the main driver procedure from the custom audit log table.
    """
    session = get_snowflake_session() # Get your Snowpark session or Connector session

    # Adjust table FQDN for your audit log table
    AUDIT_LOG_TABLE_FQDN = "YOUR_DB.YOUR_SCHEMA.T_DT_COLLECTION_AUDIT_LOG"

    # Query the custom audit log table
    # You can select specific columns, or parse the AUDIT_JSON column for more details
    # We'll select key columns for the summary, similar to original QUERY_HISTORY.
    # We remove QUERY_ID since it's not directly in this audit table's columns (unless you want to embed it).
    
    query = f"""
    SELECT
        DRIVER_RUN_UUID AS QUERY_ID, -- Map audit_log_id to QUERY_ID for consistency with old df structure
        RUN_STATUS AS EXECUTION_STATUS,
        RUN_START_TIME AS START_TIME,
        RUN_END_TIME AS END_TIME,
        TOTAL_DURATION_SEC AS TOTAL_ELAPSED_TIME_SECONDS,
        AUDIT_JSON -- Keep the full JSON if you want to drill down further in Python
    FROM
        {AUDIT_LOG_TABLE_FQDN}
    WHERE
        RUN_START_TIME >= DATEADD(day, -30, CURRENT_TIMESTAMP()) -- Last 30 days
    ORDER BY START_TIME DESC
    """
    
    # Using session.sql for Snowpark
    driver_hist_df_snowpark = session.sql(query).to_pandas()

    if not driver_hist_df_snowpark.empty:
        # Convert timestamps if not already pandas datetime (Snowpark often does this automatically)
        driver_hist_df_snowpark['START_TIME'] = pd.to_datetime(driver_hist_df_snowpark['START_TIME'])
        driver_hist_df_snowpark['END_TIME'] = pd.to_datetime(driver_hist_df_snowpark['END_TIME'])
    
    return driver_hist_df_snowpark # Return the full df, or modified df

# --- How you might use job_audit_details from AUDIT_JSON (example for context) ---
# If you load the AUDIT_JSON column into your Python DataFrame:
# from your driver_hist_df_snowpark, you would have a column 'AUDIT_JSON'
# example_audit_json = driver_hist_df_snowpark.iloc[0]['AUDIT_JSON'] # Get one JSON object
# job_audit_details = example_audit_json.get('job_audit_details', [])
# for table_audit in job_audit_details:
#     qualified_name = table_audit.get('qualified_name')
#     overall_status = table_audit.get('overall_table_status')
#     job_details = table_audit.get('job_details')
#     print(f"Table: {qualified_name}, Overall Status: {overall_status}")
#     for job_type, details in job_details.items():
#         status = details.get('status')
#         message = details.get('message', 'N/A') # Message only exists for failed ones
#         print(f"  - Job Type: {job_type}, Status: {status}, Message: {message}")
