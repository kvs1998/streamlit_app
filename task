CREATE OR REPLACE TABLE YOUR_DB.YOUR_SCHEMA.T_DT_COLLECTION_AUDIT_LOG (
    DRIVER_RUN_UUID VARCHAR PRIMARY KEY,
    RUN_STATUS VARCHAR, -- 'SUCCESS', 'FAILED_OVERALL', 'CRITICAL_FAILURE'
    RUN_START_TIME TIMESTAMP_LTZ,
    RUN_END_TIME TIMESTAMP_LTZ,
    TOTAL_DURATION_SEC NUMBER(10,3),
    TOTAL_TABLES_FOUND INTEGER,
    TOTAL_JOBS_LAUNCHED INTEGER,
    TOTAL_ASYNC_JOBS_SUCCEEDED INTEGER,
    TOTAL_ASYNC_JOBS_FAILED INTEGER,
    AUDIT_JSON VARIANT, -- Stores the complete JSON object returned by the SP
    LOAD_TIMESTAMP TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP()
);

-- Optional: Add a clustering key if you expect many rows and want faster lookups by time
ALTER TABLE YOUR_DB.YOUR_SCHEMA.T_DT_COLLECTION_AUDIT_LOG CLUSTER BY (RUN_START_TIME::DATE);



-- Replace placeholders YOUR_DB, YOUR_SCHEMA, YOUR_WAREHOUSE
-- Adjust schedule, batch size as needed.

CREATE OR REPLACE TASK YOUR_DB.YOUR_SCHEMA.DAILY_DT_COLLECTION_TASK
  WAREHOUSE = YOUR_WAREHOUSE_NAME
  SCHEDULE = 'USING CRON 0 3 * * * America/Los_Angeles'
  ALLOW_OVERLAPPING_EXECUTION = FALSE
  USER_TASK_TIMEOUT_MS = 14400000
  SUSPEND_TASK_AFTER_NUM_FAILURES = 3
  COMMENT = 'Orchestrates the collection of Dynamic Table metadata and refresh history and logs audit to T_DT_COLLECTION_AUDIT_LOG.'
  LOG_LEVEL = 'INFO'
AS
$$
    -- Snowflake Scripting block within the task
    DECLARE
        sp_result VARIANT; -- Declare a variable to hold the JSON output
    BEGIN
        -- Call your main driver stored procedure
        sp_result := CALL YOUR_DB.YOUR_SCHEMA.SP_DRIVE_ALL_DT_COLLECTION(
            P_ASYNC_BATCH_SIZE => 50, -- Your chosen batch size
            P_TIMEZONE_NAME => 'America/Los_Angeles' -- Pass the timezone to the SP
        );

        -- Insert the parsed components of the JSON output into your audit table
        INSERT INTO YOUR_DB.YOUR_SCHEMA.T_DT_COLLECTION_AUDIT_LOG (
            DRIVER_RUN_UUID,
            RUN_STATUS,
            RUN_START_TIME,
            RUN_END_TIME,
            TOTAL_DURATION_SEC,
            TOTAL_TABLES_FOUND,
            TOTAL_JOBS_LAUNCHED,
            TOTAL_ASYNC_JOBS_SUCCEEDED,
            TOTAL_ASYNC_JOBS_FAILED,
            AUDIT_JSON
        )
        SELECT
            sp_result:driver_run_uuid::VARCHAR,
            sp_result:status::VARCHAR,
            sp_result:start_time::TIMESTAMP_LTZ,
            sp_result:end_time::TIMESTAMP_LTZ,
            sp_result:total_run_duration_sec::NUMBER(10,3),
            sp_result:total_tables_found::INTEGER,
            sp_result:total_jobs_launched::INTEGER,
            sp_result:total_async_jobs_succeeded::INTEGER,
            sp_result:total_async_jobs_failed::INTEGER,
            sp_result -- Store the entire JSON object for full detail
        ;
        
        -- Optional: Add auditing for task success/failure based on sp_result:status
        -- e.g., if sp_result:status is 'FAILED_OVERALL', you could log to a separate simple error table
        -- or trigger an alert using an ERROR_INTEGRATION on the task.
        
        -- Tasks implicitly commit DMLs by default. If the insert fails, the task will fail.
    END;
$$;

-- After creation, remember to resume the task:
ALTER TASK YOUR_DB.YOUR_SCHEMA.DAILY_DT_COLLECTION_TASK RESUME;




    SELECT
        main.DRIVER_RUN_UUID,
        main.RUN_STATUS AS DRIVER_EXECUTION_STATUS, -- Status of the main driver run
        main.RUN_START_TIME,
        main.RUN_END_TIME,
        main.TOTAL_DURATION_SEC,
        
        -- Details from the flattened job_audit_details array
        job_audit.value:qualified_name::VARCHAR AS QUALIFIED_NAME,
        job_audit.value:overall_table_status::VARCHAR AS OVERALL_TABLE_STATUS_FOR_DT,
        
        -- Extract specific job_type statuses and messages
        GET(GET(job_audit.value:job_details, 'REFRESH_HISTORY'), 'status')::VARCHAR AS REFRESH_HISTORY_STATUS,
        GET(GET(job_audit.value:job_details, 'REFRESH_HISTORY'), 'message')::VARCHAR AS REFRESH_HISTORY_MESSAGE, -- Will be NULL if SUCCESS
        
        GET(GET(job_audit.value:job_details, 'METADATA_SNAPSHOT'), 'status')::VARCHAR AS METADATA_SNAPSHOT_STATUS,
        GET(GET(job_audit.value:job_details, 'METADATA_SNAPSHOT'), 'message')::VARCHAR AS METADATA_SNAPSHOT_MESSAGE, -- Will be NULL if SUCCESS
        
        -- You can add more columns from AUDIT_JSON as needed, e.g., phase_durations_sec etc.
        -- main.AUDIT_JSON -- Keep the full JSON if you want to drill down further in Python
    FROM
        {AUDIT_LOG_TABLE_FQDN} AS main,
        LATERAL FLATTEN(input => main.AUDIT_JSON:job_audit_details) AS job_audit -- Flatten the array of job_audit_details
    WHERE
        main.RUN_START_TIME >= DATEADD(day, -30, CURRENT_TIMESTAMP()) -- Last 30 days
    ORDER BY
        main.RUN_START_TIME DESC,
        QUALIFIED_NAME ASC -- Order by table name for consistency

