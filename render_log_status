# tabs/dt_health_tab.py
import streamlit as st
import pandas as pd
import plotly.express as px # For charting
from datetime import datetime, timedelta

# Assuming get_snowflake_session() is available from a utils file or defined here
# from utils.snowflake_utils import get_snowflake_session 

# --- Helper functions (re-using and adapting from previous code) ---
def format_seconds_to_readable(seconds_series, format_type):
    # This helper expects a pandas Series, but can handle single values from pd.Series([value])
    if not isinstance(seconds_series, pd.Series):
        seconds_series = pd.Series([seconds_series])

    numeric_series = pd.to_numeric(seconds_series, errors='coerce')

    if numeric_series.empty or pd.isna(numeric_series).all():
        # If all are NaN or empty, return Series of "N/A"
        return pd.Series(["N/A"] * len(numeric_series), index=numeric_series.index)

    def format_single_second(s, f_type):
        if pd.isna(s):
            return "N/A"
        
        s = float(s) # Ensure float conversion
        if s == 0: return "0s"

        if f_type == "seconds":
            return f"{s:.1f}s"
        elif f_type == "minutes":
            return f"{(s / 60):.1f}m"
        elif f_type == "hours":
            return f"{(s / 3600):.1f}h"
        elif f_type == "days":
            return f"{(s / 86400):.1f}d"
        elif f_type == "mixed":
            days = int(s // 86400)
            s_remaining = s % 86400
            hours = int(s_remaining // 3600)
            s_remaining = s_remaining % 3600
            minutes = int(s_remaining // 60)
            seconds = s_remaining % 60
            
            parts = []
            if days > 0: parts.append(f"{days}d")
            if hours > 0: parts.append(f"{hours}h")
            if minutes > 0: parts.append(f"{minutes}m")
            # Only include seconds if there are no larger units or if seconds is significant
            if seconds > 0.1 and (not parts or seconds >= 1):
                parts.append(f"{seconds:.1f}s")
            
            return " ".join(parts) if parts else "0s"
        return str(s) # Fallback

    return numeric_series.apply(lambda x: format_single_second(x, f_type=format_type))

# --- Data Fetching Functions ---

@st.cache_data(ttl=(12*60*60))
def fetch_driver_execution_history_summary():
    """
    Fetches high-level execution history summaries of the main driver procedure
    from the custom audit log table (T_DT_COLLECTION_AUDIT_LOG).
    This function focuses on overall driver run status and metrics for KPIs and trend charts.
    """
    session = st.session_state.get_snowflake_session() # Assuming session is managed in session_state

    AUDIT_LOG_TABLE_FQDN = "YOUR_DB.YOUR_SCHEMA.T_DT_COLLECTION_AUDIT_LOG" # Replace with your FQDN

    query = f"""
    SELECT
        DRIVER_RUN_UUID,
        RUN_STATUS,
        RUN_START_TIME,
        RUN_END_TIME,
        TOTAL_DURATION_SEC,
        TOTAL_TABLES_FOUND,
        TOTAL_JOBS_LAUNCHED,
        TOTAL_ASYNC_JOBS_SUCCEEDED,
        TOTAL_ASYNC_JOBS_FAILED,
        MESSAGE AS RUN_MESSAGE,
        TIMEZONE
    FROM
        {AUDIT_LOG_TABLE_FQDN}
    WHERE
        RUN_START_TIME >= DATEADD(day, -30, CURRENT_TIMESTAMP())
    ORDER BY RUN_START_TIME DESC
    """
    
    df = session.sql(query).to_pandas()

    if not df.empty:
        df['RUN_START_TIME'] = pd.to_datetime(df['RUN_START_TIME'])
        df['RUN_END_TIME'] = pd.to_datetime(df['RUN_END_TIME'])
    
    return df

@st.cache_data(ttl=(5*60)) # Cache for 5 minutes for current data freshness
def fetch_dt_tracking_data():
    """
    Fetches the current detailed tracking status for all dynamic tables
    from T_DYNAMIC_TABLE_TRACKING.
    """
    session = st.session_state.get_snowflake_session()

    TRACKING_TABLE_FQDN = "YOUR_DB.YOUR_SCHEMA.T_DYNAMIC_TABLE_TRACKING" # Replace with your FQDN

    query = f"""
    SELECT
        QUALIFIED_NAME,
        DATABASE_NAME, SCHEMA_NAME, TABLE_NAME, -- Add these if useful for filters/display
        IS_ACTIVE,
        TRACK_REFRESH_HISTORY,
        LAST_REFRESH_HISTORY_COLLECTION_TIMESTAMP,
        LAST_REFRESH_HISTORY_COLLECTION_STATUS,
        LAST_REFRESH_HISTORY_COLLECTION_MESSAGE,
        TRACK_METADATA_SNAPSHOT,
        LAST_METADATA_COLLECTION_TIMESTAMP,
        LAST_METADATA_COLLECTION_STATUS,
        LAST_METADATA_COLLECTION_MESSAGE,
        UPDATED_AT
    FROM
        {TRACKING_TABLE_FQDN}
    ORDER BY
        QUALIFIED_NAME
    """
    
    df = session.sql(query).to_pandas()

    if not df.empty:
        df['LAST_REFRESH_HISTORY_COLLECTION_TIMESTAMP'] = pd.to_datetime(df['LAST_REFRESH_HISTORY_COLLECTION_TIMESTAMP'])
        df['LAST_METADATA_COLLECTION_TIMESTAMP'] = pd.to_datetime(df['LAST_METADATA_COLLECTION_TIMESTAMP'])
        df['UPDATED_AT'] = pd.to_datetime(df['UPDATED_AT'])
    
    return df


# --- Main Render Function for the DT Health Tab ---
def render_dt_health_tab():
    st.header("Dynamic Table Health Dashboard")
    st.write("Overview of driver execution, collection status, and detailed table health.")

    # --- 1. Fetch Data ---
    driver_history_df = fetch_driver_execution_history_summary()
    tracking_data_df = fetch_dt_tracking_data()

    if driver_history_df.empty and tracking_data_df.empty:
        st.info("No data available to display the dashboard. Please check data sources.", icon="ℹ️")
        return

    # --- 2. Overall Driver Health KPIs ---
    st.subheader("Overall Driver Health KPIs")
    kpi_cols = st.columns(4) # Matching screenshot's 4 columns

    latest_run = driver_history_df.iloc[0] if not driver_history_df.empty else None
    
    total_tables_tracked = tracking_data_df['QUALIFIED_NAME'].nunique() if not tracking_data_df.empty else 0
    active_rh_tracking = tracking_data_df[
        (tracking_data_df['IS_ACTIVE'] == True) & (tracking_data_df['TRACK_REFRESH_HISTORY'] == True)
    ].shape[0] if not tracking_data_df.empty else 0
    active_metadata_tracking = tracking_data_df[
        (tracking_data_df['IS_ACTIVE'] == True) & (tracking_data_df['TRACK_METADATA_SNAPSHOT'] == True)
    ].shape[0] if not tracking_data_df.empty else 0
    
    # KPIs for screenshot
    # Total Tables Tracked (from tracking_data_df)
    # Active RH Tracking (from tracking_data_df)
    # Active Metadata Tracking (from tracking_data_df)
    # Last Driver Run Time (from driver_history_df)
    # Last Driver Duration (from driver_history_df)

    with kpi_cols[0]:
        st.metric("Total Tables Tracked", total_tables_tracked)
        st.metric("Active (RH) Tracking", active_rh_tracking)
    with kpi_cols[1]:
        st.metric("Active Tables Monitored", total_tables_tracked) # Assuming this is same as Total Tables Tracked
        st.metric("Active (Metadata) Tracking", active_metadata_tracking)
    with kpi_cols[2]:
        if latest_run is not None:
            # Assuming screenshot 'Last Driver Run Time' is RUN_END_TIME
            st.metric("Last Driver Run Time", latest_run['RUN_END_TIME'].strftime('%Y-%m-%d %H:%M:%S'))
        else:
            st.metric("Last Driver Run Time", "N/A")
    with kpi_cols[3]:
        if latest_run is not None:
            duration_formatted = format_seconds_to_readable(
                pd.Series([latest_run['TOTAL_DURATION_SEC']]), "mixed"
            ).iloc[0]
            st.metric("Last Run Duration", duration_formatted)
        else:
            st.metric("Last Run Duration", "N/A")

    st.markdown("---")

    # --- 3. Driver Execution Time Trend Chart and Collection Status Distribution ---
    chart_row_cols = st.columns([0.6, 0.4]) # Split into two columns for charts

    with chart_row_cols[0]: # Driver Execution Time Trend Chart (Left)
        st.subheader("Driver Execution Time Trend")
        st.write("Historical execution times of the main collection driver program.")
        if not driver_history_df.empty:
            driver_history_chart_data = driver_history_df.copy()
            # Rename columns for clarity in chart
            driver_history_chart_data.rename(columns={
                'RUN_START_TIME': 'Run Time',
                'TOTAL_DURATION_SEC': 'Duration (seconds)'
            }, inplace=True)

            # Checkbox for log scale
            log_scale_y = st.checkbox("Log Scale Y-axis (Driver Trend)", value=False, key="log_scale_driver_trend")
            y_axis_type = 'log' if log_scale_y else 'linear'

            fig_trend = px.line(
                driver_history_chart_data,
                x='Run Time',
                y='Duration (seconds)',
                title='Driver Program Execution Duration Over Time',
                markers=True, # Show points on the line
                log_y=log_scale_y # Apply log scale if checkbox is true
            )
            fig_trend.update_layout(hovermode="x unified") # Nice hover effect
            st.plotly_chart(fig_trend, use_container_width=True)
        else:
            st.info("No historical driver execution data for trend chart.", icon="ℹ️")

    with chart_row_cols[1]: # Collection Status Distribution (Right)
        st.subheader("Collection Status Distribution")
        st.write("Breakdown of tables by their last collection status for each type.")
        if not tracking_data_df.empty:
            # Aggregate success/fail counts from tracking_data_df for current state
            rh_status_counts = tracking_data_df.groupby('LAST_REFRESH_HISTORY_COLLECTION_STATUS').size().reset_index(name='Count')
            rh_status_counts.columns = ['Status', 'Count']
            rh_status_counts['Collection Type'] = 'Refresh History'

            md_status_counts = tracking_data_df.groupby('LAST_METADATA_COLLECTION_STATUS').size().reset_index(name='Count')
            md_status_counts.columns = ['Status', 'Count']
            md_status_counts['Collection Type'] = 'Metadata Snapshot'

            combined_status_counts = pd.concat([rh_status_counts, md_status_counts])

            # Define specific colors for statuses as per your dashboard's palette (e.g., green for SUCCESS)
            # You might need to adjust these based on your actual data's status values
            status_color_map_for_chart = {
                'SUCCESS': 'green',
                'FAILED': 'red',
                'FAILED_EXECUTION': 'red',
                'FAILED_INPUT': 'red',
                'FAILED_SUBMISSION': 'red',
                'SUSPENDED': 'orange', # Example for other states
                'UNKNOWN': 'gray'
            }

            fig_bar = px.bar(
                combined_status_counts,
                x='Count',
                y='Collection Type',
                color='Status',
                orientation='h', # Horizontal bars as in screenshot
                title='Last Collection Status by Type',
                color_discrete_map=status_color_map_for_chart, # Apply custom colors
                text='Count' # Display count on bars
            )
            fig_bar.update_traces(textposition='outside') # Place text outside bars
            fig_bar.update_layout(showlegend=True, yaxis_title=None, xaxis_title='Number of Tables') # Hide y-axis title for compactness
            st.plotly_chart(fig_bar, use_container_width=True)
        else:
            st.info("No detailed tracking data for status distribution.", icon="ℹ️")
    
    st.markdown("---")

    # --- 4. Detailed Collection Status per Table ---
    st.subheader("Detailed Collection Status per Table")
    st.write("View the last collection attempt status and messages for each dynamic table being tracked.")

    if tracking_data_df.empty:
        st.info("No detailed collection status per table available.", icon="ℹ️")
        return

    # --- Filters for the Detailed Table ---
    filter_cols = st.columns(2)

    # All unique statuses including 'All' option
    all_rh_statuses = ['All'] + sorted(tracking_data_df['LAST_REFRESH_HISTORY_COLLECTION_STATUS'].unique().tolist())
    selected_rh_status = filter_cols[0].selectbox("Filter by Refresh History Status:", options=all_rh_statuses, key="filter_rh_status")

    all_md_statuses = ['All'] + sorted(tracking_data_df['LAST_METADATA_COLLECTION_STATUS'].unique().tolist())
    selected_md_status = filter_cols[1].selectbox("Filter by Metadata Snapshot Status:", options=all_md_statuses, key="filter_md_status")

    filtered_df = tracking_data_df.copy()

    if selected_rh_status != 'All':
        filtered_df = filtered_df[filtered_df['LAST_REFRESH_HISTORY_COLLECTION_STATUS'] == selected_rh_status]
    if selected_md_status != 'All':
        filtered_df = filtered_df[filtered_df['LAST_METADATA_COLLECTION_STATUS'] == selected_md_status]

    if filtered_df.empty:
        st.info("No tables match the selected filters.", icon="ℹ️")
        return

    # --- Display Detailed Table ---
    display_df = filtered_df.copy()

    # Reformat timestamps for display
    display_df['LAST_RH_COLLECT_TIME'] = display_df['LAST_REFRESH_HISTORY_COLLECTION_TIMESTAMP'].dt.strftime('%Y-%m-%d %H:%M:%S')
    display_df['LAST_MD_COLLECT_TIME'] = display_df['LAST_METADATA_COLLECTION_TIMESTAMP'].dt.strftime('%Y-%m-%d %H:%M:%S')
    display_df['TRACKING_RECORD_LAST_UPDATED'] = display_df['UPDATED_AT'].dt.strftime('%Y-%m-%d %H:%M:%S')

    # Remove message columns as requested (and as they would be in T_DYNAMIC_TABLE_TRACKING based on current logic)
    display_df = display_df.drop(columns=[
        'LAST_REFRESH_HISTORY_COLLECTION_MESSAGE', 
        'LAST_METADATA_COLLECTION_MESSAGE'
    ], errors='ignore') 

    # Rename columns for presentation as seen in screenshot
    display_df = display_df.rename(columns={
        'QUALIFIED_NAME': 'Dynamic Table',
        'IS_ACTIVE': 'Active?',
        'TRACK_REFRESH_HISTORY': 'Track RH?',
        'LAST_REFRESH_HISTORY_COLLECTION_STATUS': 'Last RH Status',
        'TRACK_METADATA_SNAPSHOT': 'Track Metadata?',
        'LAST_METADATA_COLLECTION_STATUS': 'Last Metadata Status',
        'LAST_RH_COLLECT_TIME': 'Last RH Collect Time',
        'LAST_MD_COLLECT_TIME': 'Last Metadata Collect Time',
        'UPDATED_AT': 'Tracking Record Last Updated' # Renamed for screenshot
    })

    # Select and reorder columns for final display as seen in screenshot
    final_cols_order = [
        'Dynamic Table',
        'Active?',
        'Track RH?',
        'Last RH Collect Time',
        'Last RH Status',
        'Track Metadata?',
        'Last Metadata Collect Time',
        'Last Metadata Status',
        'Tracking Record Last Updated'
    ]

    # Ensure all selected columns exist before passing to Streamlit
    final_display_df = display_df[[col for col in final_cols_order if col in display_df.columns]]

    st.dataframe(final_display_df, use_container_width=True)
